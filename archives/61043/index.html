<!DOCTYPE html>
<html lang="zh-cn">
  <head>
    <meta charset="utf8"/>
    <meta name="viewport" content="initial-scale=1.0, width=device-width"/>
    <title>
      
      
        矩阵计算 | C'est la vie
      
    </title>
    <meta name="description" content=""/>
    <meta name="keywords" content=""/>
    
      <link rel="apple-touch-icon"
            sizes="180x180"
            href="/assets/favicon/favicon-apple-touch.png"/>
    
    
      <link rel="icon"
            type="image/png"
            sizes="32x32"
            href="/assets/favicon/favicon-32x32.png"/>
    
    
      <link rel="icon"
            type="image/png"
            sizes="16x16"
            href="/assets/favicon/favicon-16x16.png"/>
    
    
      <link rel="mask-icon"
            href="/assets/favicon/favicon-logo.svg"
            color=""/>
    
    
    
      
  <style>
    @font-face {
        font-family:sourceHanSerif;
        src: url(/font/normal.ttf);
        font-weight: normal;
    }
  </style>

  <style>
    @font-face {
        font-family:sourceHanSerif;
        src: url(/font/bold.ttf);
        font-weight: bold;
    }
  </style>


    
    <link rel="stylesheet"
          type="text/css"
          href='/css/layout.css'/>
    
    <script src="https://kit.fontawesome.com/c4ba0a82e6.js" crossorigin="anonymous"></script>
    
  <link rel="stylesheet" type="text/css" href="/css/post.css"/>
  
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"/>
  

  <meta name="generator" content="Hexo 7.0.0"></head>

  <body>
    
      <div id="search-mask" style="display:none">
  <div class="search-main" id="search-main">
    <div class="search__head">
      <div class="search-form">
        <svg t="1706347533072"
             class="icon"
             viewBox="0 0 1024 1024"
             version="1.1"
             xmlns="http://www.w3.org/2000/svg"
             p-id="7828"
             width="20"
             height="20">
          <path d="M685.6 660.336l155.152 155.168a16 16 0 0 1 0 22.624l-11.312 11.328a16 16 0 0 1-22.624 0l-158.528-158.544a289.792 289.792 0 0 1-165.152 51.36C322.336 742.256 192 611.904 192 451.12 192 290.336 322.336 160 483.136 160c160.784 0 291.12 130.336 291.12 291.136 0 82.112-33.984 156.272-88.672 209.2z m-202.464 33.92c134.272 0 243.12-108.848 243.12-243.12C726.256 316.848 617.408 208 483.136 208 348.848 208 240 316.848 240 451.136c0 134.272 108.848 243.12 243.136 243.12z" fill="#000000" p-id="7829">
          </path>
        </svg>
        <input id="search-input" placeholder="搜索文章">
        <svg t="1706361500528"
             id="search-clear"
             class="icon"
             viewBox="0 0 1024 1024"
             version="1.1"
             xmlns="http://www.w3.org/2000/svg"
             p-id="4351"
             width="20"
             height="20">
          <path d="M512 562.688l-264.2944 264.2944-50.688-50.688L461.312 512 197.0176 247.7056l50.688-50.688L512 461.312l264.2944-264.2944 50.688 50.688L562.688 512l264.2944 264.2944-50.688 50.688L512 562.688z" fill="#00" p-id="4352">
          </path>
        </svg>
      </div>
    </div>
    <div class="search__body" id="search-result"></div>
    <div class="search__foot"></div>
  </div>
</div>

    
    <div class="head">
      <div class="nav">
        <a href='/' class="nav-logo">
          <img alt="logo" height="60px" width="60px" src="/assets/favicon/favicon-logo.svg"/>
        </a>
        <input id="navBtn" type="checkbox"/>
        <div class="nav-right">
          
            <div class="search-outer">
  <div class="search" id="search-btn">
    <svg t="1706347533072"
         class="icon"
         viewBox="0 0 1024 1024"
         version="1.1"
         xmlns="http://www.w3.org/2000/svg"
         p-id="7828"
         width="20"
         height="20">
      <path d="M685.6 660.336l155.152 155.168a16 16 0 0 1 0 22.624l-11.312 11.328a16 16 0 0 1-22.624 0l-158.528-158.544a289.792 289.792 0 0 1-165.152 51.36C322.336 742.256 192 611.904 192 451.12 192 290.336 322.336 160 483.136 160c160.784 0 291.12 130.336 291.12 291.136 0 82.112-33.984 156.272-88.672 209.2z m-202.464 33.92c134.272 0 243.12-108.848 243.12-243.12C726.256 316.848 617.408 208 483.136 208 348.848 208 240 316.848 240 451.136c0 134.272 108.848 243.12 243.136 243.12z" fill="#000000" p-id="7829">
      </path>
    </svg>
    <span>搜索</span>
    <span class="search-shortcut-key">Ctrl K</span>
  </div>
</div>

          
          <div class="nav-menu">
            
              
                <a class="nav-menu-item" href="/computer">计算机</a>
              
                <a class="nav-menu-item" href="/math">数学</a>
              
                <a class="nav-menu-item" href="/physic">物理</a>
              
                <a class="nav-menu-item" href="/life">生活随笔</a>
              
            
          </div>
        </div>
        <label class="nav-btn" for="navBtn"></label>
      </div>
    </div>
    <div class="body">
      
  <article class="post-content">
    <div class="post-inner--toc">
      <div class="post-content__head">
        <div class="post-title">矩阵计算</div>
        <div class="post-info">
          
  
    <a href="/tags/%E7%9F%A9%E9%98%B5/" class="post-tag">#矩阵</a>
  


          <span class="post-date">2024-02-01</span>
        </div>
      </div>
      
        <aside class="toc-outer">
          <div class="toc-title">目录</div>
          <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#%E5%90%91%E9%87%8F-vector"><span class="post-toc-number">1.</span> <span class="post-toc-text">向量 (Vector)</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">定义</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E6%A6%82%E5%BF%B5"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">概念</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E5%90%91%E9%87%8F%E7%9A%84%E7%BA%BF%E6%80%A7%E8%BF%90%E7%AE%97"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">向量的线性运算</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E6%A8%A1%E4%B8%8E%E8%A7%92%E5%BA%A6-norm-and-angle"><span class="post-toc-number">1.4.</span> <span class="post-toc-text">模与角度 (Norm and Angle)</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E5%90%91%E9%87%8F%E5%86%85%E7%A7%AF"><span class="post-toc-number">1.4.1.</span> <span class="post-toc-text">向量内积</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E5%90%91%E9%87%8F%E7%9A%84%E6%A8%A1"><span class="post-toc-number">1.4.2.</span> <span class="post-toc-text">向量的模</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E5%90%91%E9%87%8F%E5%A4%B9%E8%A7%92"><span class="post-toc-number">1.4.3.</span> <span class="post-toc-text">向量夹角</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E5%90%91%E9%87%8F%E6%AD%A3%E4%BA%A4orthogonal"><span class="post-toc-number">1.4.4.</span> <span class="post-toc-text">向量正交（orthogonal）</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#%E6%B3%B0%E5%8B%92%E5%85%AC%E5%BC%8F%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%8C%96"><span class="post-toc-number">2.</span> <span class="post-toc-text">泰勒公式与线性化</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E5%AE%9A%E4%B9%89-1"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">定义</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#jacobian-%E7%9F%A9%E9%98%B5"><span class="post-toc-number">3.</span> <span class="post-toc-text">Jacobian 矩阵</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#n-%E7%BB%B4%E6%AC%A7%E6%B0%8F%E7%A9%BA%E9%97%B4"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">n 维欧氏空间</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E5%AE%9A%E4%B9%89-2"><span class="post-toc-number">3.1.1.</span> <span class="post-toc-text">定义</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E5%90%91%E9%87%8F%E5%87%BD%E6%95%B0%E4%B8%8E%E5%AE%9E%E5%80%BC%E5%87%BD%E6%95%B0"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">向量函数与实值函数</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E6%A2%AF%E5%BA%A6"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">梯度</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E5%88%86%E7%B1%BB"><span class="post-toc-number">3.3.1.</span> <span class="post-toc-text">分类</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E8%BF%90%E7%AE%97%E6%B3%95%E5%88%99"><span class="post-toc-number">3.3.2.</span> <span class="post-toc-text">运算法则</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#jacobian-%E7%9F%A9%E9%98%B5-1"><span class="post-toc-number">3.3.3.</span> <span class="post-toc-text">Jacobian 矩阵</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#hessian-%E7%9F%A9%E9%98%B5"><span class="post-toc-number">4.</span> <span class="post-toc-text">Hessian 矩阵</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E7%90%86%E8%AE%BA%E5%92%8C-hmm"><span class="post-toc-number">5.</span> <span class="post-toc-text">贝叶斯决策理论和 HMM</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E7%90%86%E8%AE%BA"><span class="post-toc-number">5.1.</span> <span class="post-toc-text">贝叶斯决策理论</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86"><span class="post-toc-number">5.1.1.</span> <span class="post-toc-text">贝叶斯定理</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#mlemap-%E5%92%8C-bayesian"><span class="post-toc-number">5.1.2.</span> <span class="post-toc-text">MLE、MAP 和 Bayesian</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1maximum-likelihood-estimation-mle"><span class="post-toc-number">5.1.2.1.</span> <span class="post-toc-text">最大似然估计（Maximum
likelihood estimation, MLE）</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1maximum-a-posteriori-estimationmap"><span class="post-toc-number">5.1.2.2.</span> <span class="post-toc-text">最大后验估计（Maximum
a posteriori estimation,MAP）</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1bayesian-estimation"><span class="post-toc-number">5.1.2.3.</span> <span class="post-toc-text">贝叶斯估计（Bayesian
estimation）</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95"><span class="post-toc-number">5.1.3.</span> <span class="post-toc-text">蒙特卡洛方法</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8Bhidden-markov-model-hmm"><span class="post-toc-number">5.2.</span> <span class="post-toc-text">隐马尔科夫模型（Hidden
Markov Model, HMM）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E5%8F%98%E9%87%8F"><span class="post-toc-number">5.2.1.</span> <span class="post-toc-text">变量</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E7%8A%B6%E6%80%81"><span class="post-toc-number">5.2.2.</span> <span class="post-toc-text">状态</span></a></li></ol></li></ol></li></ol>
          <a href="#" class="toc-top">回到顶部</a>
        </aside>
      
      <div class="post-content__body">
        
          <div class="post-gallery">
            
          </div>
        
        <h1 id="向量-vector">向量 (Vector)</h1>
<h2 id="定义">定义</h2>
<p>数域 <span class="math inline">\(F\)</span> 上的 <span
class="math inline">\(n\)</span> 个数 <span
class="math inline">\(a_1,a_2,...,a_n\)</span> 所组成的有序数组 <span
class="math inline">\([a_1,a_2,...,a_n]\)</span> 称为 <span
class="math inline">\(n\)</span> 维向量，其中第 <span
class="math inline">\(i\)</span> 个数称为该向量的第 <span
class="math inline">\(i\)</span> 个分量. 通常用加粗小写字母 <span
class="math inline">\(\mathbf{a}\)</span>，<span
class="math inline">\(\mathbf{b}\)</span> 表示.</p>
<h2 id="概念">概念</h2>
<ul>
<li>行向量：<span
class="math inline">\(\mathbf{\alpha}=(a_1,a_2,...,a_n)\)</span></li>
<li>列向量：<span
class="math inline">\(\mathbf{\beta}=\left(\begin{matrix}
  b_1 \\
  b_2 \\
  \vdots \\
  b_n
\end{matrix}\right)=(b_1,b_2,...,b_n)^T\)</span></li>
</ul>
<h2 id="向量的线性运算">向量的线性运算</h2>
<p>对向量 <span
class="math inline">\(\mathbf{a}=(a_1,a_2,...,a_n)\)</span> 和 <span
class="math inline">\(\mathbf{b}=(b_1,b_2, b_n)\)</span>，有</p>
<ul>
<li>相等</li>
</ul>
<p><span class="math display">\[
\mathbf{a}=\mathbf{b} \Leftrightarrow a_i=b_i(i=1,2,...,n)
\]</span></p>
<ul>
<li>加法</li>
</ul>
<p><span class="math display">\[
\mathbf{a}\pm\mathbf{b}=(a_1\pm b_1,a_2\pm b_2,...,a_n\pm b_n)
\]</span></p>
<ul>
<li>数乘</li>
</ul>
<p><span class="math display">\[
k\mathbf{a}=(ka_1,ka_2,...,ka_n)
\]</span></p>
<ul>
<li>运算律</li>
</ul>
<ol type="1">
<li><span
class="math inline">\(\mathbf{a}+\mathbf{b}=\mathbf{b}+\mathbf{a}\)</span></li>
<li><span
class="math inline">\((\mathbf{a}+\mathbf{b})+\mathbf{c}=\mathbf{a}+(\mathbf{b}+\mathbf{c})\)</span></li>
<li><span
class="math inline">\(k(\mathbf{a}+\mathbf{b})=k\mathbf{a}+k\mathbf{b}\)</span></li>
<li><span
class="math inline">\((k+l)\mathbf{a}=k\mathbf{a}+l\mathbf{a}\)</span></li>
</ol>
<h2 id="模与角度-norm-and-angle">模与角度 (Norm and Angle)</h2>
<h3 id="向量内积">向量内积</h3>
<ul>
<li>定义</li>
</ul>
<p>给定 <span class="math inline">\(\mathbb{R}^n\)</span> 中的向量 <span
class="math inline">\(\mathbf{a}=(a_1,a_2,...,a_n)^T,
\mathbf{b}=(b_1,b_2, b_n)^T\)</span>，称实数</p>
<p><span class="math display">\[
\sum_{i=1}^na_ib_i=a_1b_1+a_2b_2+...+a_nb_n
\]</span></p>
<p>为 <span class="math inline">\(\mathbf{a}\)</span> 和 <span
class="math inline">\(\mathbf{b}\)</span> 的内积，记为 <span
class="math inline">\(\mathbf{a}^T\mathbf{b}\)</span> .</p>
<ul>
<li>运算性质</li>
</ul>
<p>设 <span class="math inline">\(\alpha, \beta, \gamma\)</span> 为
<span class="math inline">\(\mathbb{R}^n\)</span> 中的向量，<span
class="math inline">\(k\)</span> 为实数：</p>
<ol type="1">
<li><span
class="math inline">\(\alpha^T\beta=\beta^T\alpha\)</span></li>
<li><span
class="math inline">\((k\alpha)^T\beta=k(\alpha^T\beta)=\alpha^T(k\beta)\)</span></li>
<li><span
class="math inline">\((\alpha+\beta)^T\gamma=\alpha^T\gamma+\beta^T\gamma\)</span></li>
<li><span
class="math inline">\(\alpha^T\alpha=0\Leftrightarrow\alpha=\mathbf{0}\)</span></li>
</ol>
<h3 id="向量的模">向量的模</h3>
<ul>
<li>定义</li>
</ul>
<p>给定 <span class="math inline">\(\mathbb{R}^n\)</span> 中的向量 <span
class="math inline">\(\mathbf{a}=(a_1,a_2,...,a_n)^T,
\mathbf{b}=(b_1,b_2, b_n)^T\)</span>，称实数</p>
<p><span class="math display">\[
\sqrt{\mathbf{a}^T\mathbf{a}}=\sqrt{a_1^2+a_2^2+...+a_n^2}=\sqrt{\sum_{i=1}^na_i^2}
\]</span></p>
<p>​为向量 <span class="math inline">\(\mathbf{a}\)</span> 的长度，记作
<span class="math inline">\(||\mathbf{a}||\)</span></p>
<ul>
<li>性质</li>
</ul>
<ol type="1">
<li>非负性：<span class="math inline">\(||\mathbf{a}||\geq
0\)</span></li>
<li>齐次性：<span
class="math inline">\(||k\mathbf{a}||=|k|\cdot||\mathbf{a}||\)</span></li>
<li>柯西不等式：</li>
</ol>
<p><span class="math display">\[
\begin{align*}
    &amp;\left|\sum_{i=1}^na_ib_i\right|\leq\sqrt{\sum_{i=1}^na_i^2}\sqrt{\sum_{i=1}^nb_i^2}
\\
    \Leftrightarrow \quad&amp;
|\mathbf{a}^T\mathbf{b}|\leq||\mathbf{a}||\cdot||\mathbf{b}||
\end{align*}
\]</span></p>
<p>等号成立当且仅当 <span class="math inline">\(\mathbf{a}\)</span> 和
<span class="math inline">\(\mathbf{b}\)</span> 线性相关。</p>
<ol start="4" type="1">
<li>若向量 <span class="math inline">\(\mathbf{a}\)</span> 的模为 <span
class="math inline">\(1\)</span>，则称 <span
class="math inline">\(\mathbf{a}\)</span> 为单位向量。设非零向量 <span
class="math inline">\(\mathbf{a}\)</span>，令 <span
class="math inline">\(\eta=\frac{\mathbf{a}}{||\mathbf{a}||}\)</span>，则
<span class="math inline">\(||\eta||=1\)</span>，且方向与 <span
class="math inline">\(\mathbf{a}\)</span> 相同。</li>
</ol>
<h3 id="向量夹角">向量夹角</h3>
<img src="/archives/61043/angle.png" class="" title="angle">
<ul>
<li>定义</li>
</ul>
<p>若 <span class="math inline">\(x=(x_1,x_2)^T,
y=(y_1,y_2)^T\)</span>，则</p>
<p><span class="math display">\[
\begin{align}
    &amp;L_x=||x||=\sqrt{x_1^2+x_2^2},\quad L_y=||y||=\sqrt{y_1^2+y_2^2}
\\
    &amp;\cos{\theta_1}=\frac{x_1}{||x||},\quad
\cos{\theta_2}=\frac{y_1}{||y||} \\
    &amp;\sin{\theta_1}=\frac{x_2}{||x||},\quad
\sin{\theta_2}=\frac{y_2}{||y||} \\
    \cos{\theta}&amp;=\cos{(\theta_2-\theta_1)}=\cos{\theta_2}\cos{\theta_1}+\sin{\theta_2}\sin{\theta_1}\notag
\\
    &amp;=\frac{x_1y_1+x_2y_2}{||x||\cdot||y||}=\frac{x^Ty}{||x||\cdot||y||}
\end{align}
\]</span></p>
<h3 id="向量正交orthogonal">向量正交（orthogonal）</h3>
<ul>
<li>定义</li>
</ul>
<p>若两个 <span class="math inline">\(n\)</span> 维向量 <span
class="math inline">\(\mathbf{a}\)</span> 和 <span
class="math inline">\(\mathbf{b}\)</span> 满足</p>
<p><span class="math display">\[
\mathbf{a}^T\mathbf{b}=a_1b_1+a_2b_2+...+a_nb_n=0
\]</span></p>
<p>则称 <span class="math inline">\(\mathbf{a}\)</span> 和 <span
class="math inline">\(\mathbf{b}\)</span> 正交.</p>
<ul>
<li>性质</li>
</ul>
<ol type="1">
<li>零向量与任何向量都正交</li>
<li>与自身正交的向量只有零向量</li>
</ol>
<ul>
<li>正交向量组</li>
</ul>
<p>若非零向量组 <span
class="math inline">\((\mathbf{a}_1,\mathbf{a}_2,...,\mathbf{a}_n)\)</span>
中，向量两两正交，即 <span
class="math inline">\(\mathbf{a}_i^T\mathbf{a}_j=0 (i\neq
j)\)</span>，则称该向量组为正交向量组.</p>
<h1 id="泰勒公式与线性化">泰勒公式与线性化</h1>
<h2 id="定义-1">定义</h2>
<p>若函数 <span class="math inline">\(f(x)\)</span> 在 <span
class="math inline">\(x_0\)</span> 处有 <span
class="math inline">\(n+1\)</span> 阶导数，则存在 <span
class="math inline">\(x_0\)</span> 的一个邻域，对于该邻域内任一 <span
class="math inline">\(x\)</span>，有</p>
<p><span class="math display">\[
f(x)=f(x_0)+f&#39;(x_0)(x-x_0)+\frac{f^{(2)}(x_0)}{2!}(x-x_0)^2+…+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+R_n(x)
\]</span></p>
<p>该式子称为函数 <span class="math inline">\(f(x)\)</span> 在 <span
class="math inline">\(x_0\)</span> 的泰勒公式. 其中，<span
class="math inline">\(R_n(x)=o((x-x_0)^n)\)</span>
称为拉格朗日余项，表示用 <span class="math inline">\(n\)</span>
次泰勒多项式来近似表示 <span class="math inline">\(f(x)\)</span>
所产生的误差. <span
class="math inline">\(R_n(x)\)</span>的具体表示为：</p>
<p><span class="math display">\[
R_n(x)=\frac{f^{n+1}(\xi)}{(n+1)!}(x-x_0)^{n+1}\quad (\xi \in (x_0,x))
\]</span></p>
<p>当 <span class="math inline">\(n=0\)</span>
时，是拉格朗日中值公式：</p>
<p><span class="math display">\[
f(x)=f(x_0)+f&#39;(\xi)(x-x_0)\quad (\xi \in (x_0,x))
\]</span></p>
<p>当 <span class="math inline">\(x_0=0\)</span>
时，是麦克劳林公式：</p>
<p><span class="math display">\[
f(x)=f(0)+f&#39;(\xi)x+\frac{f^{(2)}(0)}{2!}x^2+…+\frac{f^{(n)}(0)}{n!}x^n+R_n(x)\quad
(\xi \in (x_0,x))
\]</span></p>
<h1 id="jacobian-矩阵">Jacobian 矩阵</h1>
<h2 id="n-维欧氏空间">n 维欧氏空间</h2>
<h3 id="定义-2">定义</h3>
<p>所有 <span class="math inline">\(n\)</span> 个有序实数组 <span
class="math inline">\((x_1,x_2,…,x_n)\)</span> 的全体称为 <span
class="math inline">\(n\)</span> 维向量空间，或简称 <span
class="math inline">\(n\)</span> 维空间，其中每个有序实数组称为 <span
class="math inline">\(n\)</span>
维空间中的一个向量（或一个点），记作</p>
<p><span class="math display">\[
x=(x_1,x_2,…,x_n)^T
\]</span></p>
<p>设 <span class="math inline">\(x=(x_1,x_2,…,x_n)^T\)</span> 与 <span
class="math inline">\(y=(y_1,y_2,…,y_n)^T\)</span> 是 <span
class="math inline">\(n\)</span> 维空间中的任意两个向量，则向量 <span
class="math inline">\(x\)</span> 与 <span
class="math inline">\(y\)</span> 的内积定义为：</p>
<p><span class="math display">\[
x^Ty=x_1y_1+x_2y_2+…+x_ny_n
\]</span></p>
<p>定义了内积的 <span class="math inline">\(n\)</span> 维空间叫做 <span
class="math inline">\(n\)</span> 维欧几里得空间（简称 <span
class="math inline">\(n\)</span> 维欧式空间），记作 <span
class="math inline">\(R^n\)</span>，利用内积定义向量 <span
class="math inline">\(x \in R^n\)</span> 的模为：</p>
<p><span class="math display">\[
|x|=\sqrt{x^Tx}=\sqrt{\sum_{i=1}^nx_i^2}
\]</span></p>
<h2 id="向量函数与实值函数">向量函数与实值函数</h2>
<ul>
<li>实值函数</li>
</ul>
<p>即 <span class="math inline">\(n\)</span>
维空间中的点集到实数集的映射（<span class="math inline">\(R^n
\rightarrow R\)</span></p>
<ul>
<li>向量函数</li>
</ul>
<p>即 <span class="math inline">\(n\)</span> 维欧氏空间中的点集到 <span
class="math inline">\(m\)</span> 维欧氏空间点集的映射（<span
class="math inline">\(R^n \rightarrow R^m\)</span>） 一般地，当 <span
class="math inline">\(f_1,f_2,…,f_m\)</span> 为 <span
class="math inline">\(f\)</span> 的分量函数（或坐标函数）时，可写作</p>
<p><span class="math display">\[
f(x)=\left(\begin{array}{c}
f_1 \\
\vdots \\
f_m
\end{array}\right)=\left(\begin{array}{c}
f_1(x) \\
\vdots \\
f_m(x)
\end{array}\right)=\left(\begin{array}{c}
f_1(x_1,x_2,…,x_n) \\
\vdots \\
f_m(x_1,x_2,…,x_n)
\end{array}\right)
\]</span></p>
<p>两个相同维数的向量函数 <span class="math inline">\(f\)</span> 和
<span class="math inline">\(g\)</span>
在相同的定义域上的和（差）函数为：</p>
<p><span class="math display">\[
f \pm g =\left(\begin{array}{c}
f_1 \pm g_1 \\
\vdots \\
f_n \pm g_n
\end{array}\right)
\]</span></p>
<p>一个实值函数 <span class="math inline">\(\alpha\)</span> 向量函数
<span class="math inline">\(f\)</span>
在相同的定义域上的乘积函数为：</p>
<p><span class="math display">\[
\alpha f =\left(\begin{array}{c}
\alpha f_1 \\
\vdots \\
\alpha f_n
\end{array}\right)
\]</span></p>
<p>两个向量函数 <span class="math inline">\(f\)</span> 和 <span
class="math inline">\(h\)</span> 的复合函数为：</p>
<p><span class="math display">\[
h \cdot f =\left(\begin{array}{c}
h_1 \cdot f \\
\vdots \\
h_n \cdot f
\end{array}\right)
\]</span></p>
<h2 id="梯度">梯度</h2>
<h3 id="分类">分类</h3>
<p>根据自变量和因变量的不同可分为：</p>
<ol type="1">
<li><p>自变量为实向量的标量函数 <span class="math inline">\(f\)</span>
关于向量的梯度： 函数 <span class="math inline">\(f\)</span> 关于行向量
<span class="math inline">\(x^T\)</span> 的梯度为：</p>
<p><span class="math display">\[
\frac{\partial f}{\partial x^T} = [\frac{\partial f}{\partial
x_1},...,\frac{\partial f}{\partial x_n}] = \nabla_{x^T}f(x)
\]</span></p>
<p>函数 <span class="math inline">\(f\)</span> 关于列向量 <span
class="math inline">\(x\)</span> 的梯度计算公式为：</p>
<p><span class="math display">\[
\frac{\partial f}{\partial x} = [\frac{\partial f}{\partial
x_1},...,\frac{\partial f}{\partial x_n}]^T = \nabla_{x}f(x)
\]</span></p>
<p>若 <span class="math inline">\(f\)</span> 为常数，则对应的梯度为
<span class="math inline">\(\mathbf{0}\)</span>.</p></li>
<li><p>自变量为实向量（<span class="math inline">\(x_{n \times
1}\)</span>）的向量函数 <span class="math inline">\(f\)</span>
关于向量的梯度：</p>
<p><span class="math display">\[
f \in R^m,
\nabla_xf = \left[\begin{matrix}
\frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_2}{\partial
x_1} &amp; \cdots &amp; \frac{\partial f_m}{\partial x_1} \\
\frac{\partial f_1}{\partial x_2} &amp; \frac{\partial f_2}{\partial
x_2} &amp; \cdots &amp; \frac{\partial f_m}{\partial x_2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial f_1}{\partial x_n} &amp; \frac{\partial f_2}{\partial
x_n} &amp; \cdots &amp; \frac{\partial f_m}{\partial x_n}
\end{matrix}\right]
= \frac{\partial f}{\partial x}
\]</span></p></li>
<li><p>自变量为矩阵（<span class="math inline">\(x_{m \times
n}\)</span>）的标量函数 <span class="math inline">\(f\)</span>
关于矩阵的梯度：</p>
<p><span class="math display">\[
f \in R,
\nabla_Xf = \left[\begin{matrix}
\frac{\partial f_1}{\partial x_{11}} &amp; \frac{\partial f_2}{\partial
x_{12}} &amp; \cdots &amp; \frac{\partial f_m}{\partial x_{1n}} \\
\frac{\partial f_1}{\partial x_{21}} &amp; \frac{\partial f_2}{\partial
x_{22}} &amp; \cdots &amp; \frac{\partial f_m}{\partial x_{2n}} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial f_1}{\partial x_{m1}} &amp; \frac{\partial f_2}{\partial
x_{m2}} &amp; \cdots &amp; \frac{\partial f_m}{\partial x_{mn}}
\end{matrix}\right]
= \frac{\partial f}{\partial X}
\]</span></p></li>
</ol>
<h3 id="运算法则">运算法则</h3>
<ul>
<li>加法法则：若 <span class="math inline">\(f(x),g(x)\)</span> 均为
<span class="math inline">\(x\)</span> 的实值函数，则</li>
</ul>
<p><span class="math display">\[
\frac{\partial[pf(x)+qg(x)]}{\partial x}=p\frac{\partial f(x)}{\partial
x}+q\frac{\partial g(x)}{\partial x}
\]</span></p>
<ul>
<li>乘法法则：若 <span class="math inline">\(f(x),g(x)\)</span> 均为
<span class="math inline">\(x\)</span> 的实值函数，则</li>
</ul>
<p><span class="math display">\[
\frac{\partial[f(x)g(x)]}{\partial x}=f(x)\frac{\partial g(x)}{\partial
x}+g(x)\frac{\partial f(x)}{\partial x}
\]</span></p>
<ul>
<li>除法法则：若 <span class="math inline">\(f(x)\)</span> 为向量 <span
class="math inline">\(x\)</span> 的向量值函数，则</li>
</ul>
<p><span class="math display">\[
\frac{\partial[f(x)/g(x)]}{\partial
x}=\frac{1}{g^2(x)}\left[g(x)\frac{\partial f(x)}{\partial
x}-f(x)\frac{\partial g(x)}{\partial x}\right]
\]</span></p>
<ul>
<li>链式法则：若<span class="math inline">\(g(x)\)</span>为<span
class="math inline">\(x\)</span>的向量值函数，则</li>
</ul>
<p><span class="math display">\[
\frac{\partial g(f(x))}{\partial x}=\frac{\partial g(f)}{\partial
f}\frac{\partial f(x)}{\partial x}
\]</span></p>
<ul>
<li>常见类型的梯度 <span class="math display">\[
\begin{align*}
  \nabla(x^Tx)=\frac{\partial x^Tx}{\partial x} = 2x,\quad
\nabla(a^Tx)=\frac{\partial a^Tx}{\partial x} = a,\\
  \nabla(x^TA)=\frac{\partial x^TA}{\partial x} = A,\quad
\nabla(x^TAx)=\frac{\partial x^TAx}{\partial x} = (A+A^T)x
\end{align*}
\]</span></li>
</ul>
<h3 id="jacobian-矩阵-1">Jacobian 矩阵</h3>
<p>若函数 <span class="math inline">\(f(x):\mathbb{R}^n \rightarrow
\mathbb{R}^m\)</span>，有：</p>
<p><span class="math display">\[
x=[x_1,x_2,…,x_n]^T,
f(x)=\left[\begin{array}{c}
f_1(x_1,x_2,…,x_n) \\
f_2(x_1,x_2,…,x_n) \\
\vdots \\
f_m(x_1,x_2,…,x_n)
\end{array}\right]
\]</span></p>
<p>则 Jacobian 矩阵 <span class="math inline">\(J(x)\)</span>
可以写为：</p>
<p><span class="math display">\[
f \in \mathbb{R}^{m \times 1},
J(x) =\left[\begin{matrix}
\frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_1}{\partial
x_2} &amp; \cdots &amp; \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} &amp; \frac{\partial f_2}{\partial
x_2} &amp; \cdots &amp; \frac{\partial f_2}{\partial x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial f_m}{\partial x_1} &amp; \frac{\partial f_m}{\partial
x_2} &amp; \cdots &amp; \frac{\partial f_m}{\partial x_n}
\end{matrix}\right]
= \nabla_x^T f
\]</span></p>
<p>Jacobian 矩阵表现了向量函数的最佳线性逼近，也即</p>
<p><span class="math display">\[
f(x) \approx f(p) + J(p)(x-p)
\]</span></p>
<h1 id="hessian-矩阵">Hessian 矩阵</h1>
<p>若 <span class="math inline">\(f:\mathbb{R}^n \rightarrow
\mathbb{R}\)</span> 为二次可导函数，<span
class="math inline">\(x=[x_1,x_2,…,x_n]^T\)</span>，则 <span
class="math inline">\(f\)</span> 的Hessian矩阵为：</p>
<p><span class="math display">\[
H(f)=\left[\begin{matrix}
\frac{\partial^2 f}{\partial x_1 \partial x_1} &amp; \frac{\partial^2
f}{\partial x_1 \partial x_2} &amp; \cdots &amp; \frac{\partial^2
f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2
f}{\partial x_2 \partial x_2} &amp; \cdots &amp; \frac{\partial^2
f}{\partial x_2 \partial x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \frac{\partial^2
f}{\partial x_n \partial x_2} &amp; \cdots &amp; \frac{\partial^2
f}{\partial x_n \partial x_n}
\end{matrix}\right]
=\nabla_x^2f
\]</span></p>
<p>Hessian矩阵使用了函数的二阶信息，常用于解决优化问题。当 <span
class="math inline">\(f\)</span>
的二阶混合偏导数连续时，它是一个对称矩阵。这时，<span
class="math inline">\(f\)</span> 在 <span
class="math inline">\(x_0\)</span> 的二阶泰勒公式可简单地写成：</p>
<p><span class="math display">\[
f(x)=f(x_0)+f&#39;(x_0)(x-x_0)+\frac{1}{2}(x-x_0)^Tf^2(x_0)(x-x_0)+o(|x-x_0|^2)
\]</span></p>
<h1 id="贝叶斯决策理论和-hmm">贝叶斯决策理论和 HMM</h1>
<h2 id="贝叶斯决策理论">贝叶斯决策理论</h2>
<h3 id="贝叶斯定理">贝叶斯定理</h3>
<p>设 <span class="math inline">\(n\)</span> 个事件 <span
class="math inline">\(A_1,A_2,…,A_n\)</span> 两两互斥，<span
class="math inline">\(A_1+A_2+…+A_n=\Omega\)</span>（满足这两个条件的事件组称为一个完备事件组），且
<span class="math inline">\(P(A_i)&gt;0(i=1,2,…,n)\)</span> 则</p>
<p><span class="math display">\[
P(A_i|B)=\frac{P(A_i)P(B|A_i)}{\sum_{j=1}^nP(A_i)P(B|A_j)}
\]</span></p>
<p><span class="math inline">\(P(A_i|B)\)</span> 是在事件 <span
class="math inline">\(B\)</span> 发生的条件下，某个原因 <span
class="math inline">\(A_i\)</span> 发生的概率，称为后验概率，称 <span
class="math inline">\(P(A_i)\)</span> 为先验概率.</p>
<h3 id="mlemap-和-bayesian">MLE、MAP 和 Bayesian</h3>
<p>令：</p>
<ul>
<li><span class="math inline">\(D={(x_1,y_1),…,(x_n,y_n)}\)</span>
表示训练样本集</li>
<li><span class="math inline">\(\theta\)</span> 表示模型所有的参数</li>
<li><span class="math inline">\(x^*\)</span>
表示一个新的样本（训练完的模型需要做预测），<span
class="math inline">\(\hat y\)</span> 表示模型对新样本的预测值</li>
</ul>
<h4
id="最大似然估计maximum-likelihood-estimation-mle">最大似然估计（Maximum
likelihood estimation, MLE）</h4>
<ul>
<li>在给定一个模型参数的情况下，计算样本集的概率 <span
class="math inline">\(p(D|\theta)\)</span></li>
<li>目标是找到最优的参数 <span
class="math inline">\(\theta^*\)</span>使得<span
class="math inline">\(p(D|\theta)\)</span> 最大化</li>
<li>预测 <span class="math inline">\(p(\hat y|x^,\theta^)\)</span></li>
</ul>
<p>目标：</p>
<p><span class="math display">\[
\theta^{MLE}=\arg\max\theta p(D|\theta)
\]</span></p>
<p>要找到最优参数 <span class="math inline">\(\theta^{MLE}\)</span> 使得
<span class="math inline">\(p(D|\theta)\)</span>
最大化，通常可通过令导数等于0等到：</p>
<p><span class="math display">\[
\frac{\partial p(D|\theta)}{\partial \theta}=0
\]</span></p>
<p>MLE 未考虑到先验，且容易造成过拟合.</p>
<h4
id="最大后验估计maximum-a-posteriori-estimationmap">最大后验估计（Maximum
a posteriori estimation,MAP）</h4>
<ul>
<li>在给定样本的情况下，计算参数的概率 <span
class="math inline">\(p(\theta|D)\)</span></li>
<li>目标是找到最优的参数 <span class="math inline">\(\theta^*\)</span>
使得 <span class="math inline">\(p(\theta|D)\)</span> 最大化</li>
<li>预测 <span class="math inline">\(p(\hat y|x^,\theta^)\)</span></li>
</ul>
<p>目标：</p>
<p><span class="math display">\[
\theta^{MAP}=\arg\max\theta p(\theta|D)
\]</span></p>
<p>根据贝叶斯公式有：</p>
<p><span class="math display">\[
p(\theta|D)=\frac{p(D|\theta)p(\theta)}{p(D)}
\]</span></p>
<p>目标可以转为：</p>
<p><span class="math display">\[
\theta^{MAP}=\arg\max\theta p(D|\theta)p(\theta)
\]</span></p>
<p>进一步取对数形式：</p>
<p><span class="math display">\[
\theta^{MAP}=\arg\max\theta \log p(D|\theta)+\log p(\theta)
\]</span></p>
<p>MAP 与 MLE 最大不同在于 <span
class="math inline">\(p(\theta)\)</span> 项，MAP 解决了 MLE
缺乏先验的缺点.</p>
<h4 id="贝叶斯估计bayesian-estimation">贝叶斯估计（Bayesian
estimation）</h4>
<ul>
<li>在给定样本的情况下，计算参数的分布 <span
class="math inline">\(p(\theta|D)\)</span></li>
<li>预测 <span class="math inline">\(p(\hat y|x^*,D)=\int_\theta p(\hat
y|x^*,\theta)p(\theta|D)d\theta\)</span></li>
</ul>
<p>目标：</p>
<p>对于一个新样本<span
class="math inline">\(x^*\)</span>，模型的预测为：</p>
<p><span class="math display">\[
p(\hat y|x^*,D)=\int_\theta p(\hat y|x^*,\theta)p(\theta|D)d\theta
\]</span></p>
<p>需要计算后验概率 <span
class="math inline">\(p(\theta|D)\)</span>：</p>
<p><span class="math display">\[
p(\theta|D)=\frac{p(D|\theta)p(\theta)}{\int
p(D|\theta)p(\theta)d\theta}
\]</span></p>
<p>后验概率需要近似计算.</p>
<img src="/archives/61043/MLE_MAP_Bay.png" class="" title="MLE_MAP_Bay">
<h3 id="蒙特卡洛方法">蒙特卡洛方法</h3>
<p>通过随机抽样方法，以随机事件出现的频率估计其概率，或者以抽样的数字特征估算随机变量的数字特征，并将其作为问题的解。</p>
<h2 id="隐马尔科夫模型hidden-markov-model-hmm">隐马尔科夫模型（Hidden
Markov Model, HMM）</h2>
<ul>
<li>一种简单的有向图模型，常用于处理时间序列数据</li>
<li>在多个领域应用广泛：语音识别、自然语言处理、词性标注等等</li>
</ul>
<h3 id="变量">变量</h3>
<p>HMM的变量分为两种：</p>
<ul>
<li>因变量 <span class="math inline">\(y \in Y\)</span> ：在<span
class="math inline">\(i\)</span>时刻系统所处的状态（通常离散型）</li>
<li>观察变量 <span class="math inline">\(x \in X\)</span> ：在<span
class="math inline">\(i\)</span>时刻所处的观察值（离散、连续）</li>
</ul>
<img src="/archives/61043/HMM.png" class="" title="HMM">
<h3 id="状态">状态</h3>
<p>下一个状态仅由当前状态决定，与其他状态变量无关.</p>
<p><span class="math inline">\(x\)</span> 在给定 <span
class="math inline">\(y\)</span> 的情况下条件独立，HMM
中因变量和观察变量的联合分布可以写为</p>
<p><span class="math display">\[
\begin{align*}
    p(\textbf{x}^n,\textbf{y}^n)=p(y_1)p(x_1|y_1)\prod_{t=2}^Tp(x_t|y_t)p(y_t|y_{t-1})\\
    \textbf{x}^n=(x_1,x_2,...,x_T),\quad \textbf{y}^n=(y_1,y_2,...,y_T)
\end{align*}
\]</span></p>

      </div>
    </div>
    
      <script src='https://unpkg.com/mermaid@latest/dist/mermaid.min.js'></script>
      <script>
        if (window.mermaid) {
          mermaid.initialize([object Object]);
        }
      </script>
    
  </article>
  <div class="post__foot">
    
      <div class="like-author">
  <input type="checkbox" id="likeCode" />
  <div class="author-face">
    <img height="100px"
         width="100px"
         id="front-face"
         alt="author face"
         src="/assets/reward/avatar.png" />
    <img height="100px"
         width="100px"
         id="back-face"
         alt="like code"
         src="/assets/reward/sponsor.png" />
  </div>
  <div class="like-text">
    给作者倒杯卡布奇诺
  </div>
  <label for="likeCode" class="like-btn">
    <svg viewBox="0 0 1024 1024"
         width="20px"
         style="margin-right: 10px"
         height="20px">
      <path d="M466.88 908.96L113.824 563.296a270.08 270.08 0 0 1 0-387.392c108.8-106.56 284.896-106.56 393.696 0 1.504 1.472 2.976 2.944 4.448 4.48 1.472-1.536 2.944-3.008 4.448-4.48 108.8-106.56 284.896-106.56 393.696 0a269.952 269.952 0 0 1 34.016 347.072l-387.392 385.6a64 64 0 0 1-89.92 0.384z" p-id="13650" fill="#ee4242" />
    </svg>
    喜欢作者
  </label>
</div>

    
    <div class="post-nav">
  
    <a class="post-nav-item-left" href="/archives/47690/">
      <div class="text-align">
        <svg t="1670570876164"
             class="icon"
             viewBox="0 0 1024 1024"
             width="16"
             height="16">
          <path d="M384 512L731.733333 202.666667c17.066667-14.933333 19.2-42.666667 4.266667-59.733334-14.933333-17.066667-42.666667-19.2-59.733333-4.266666l-384 341.333333c-10.666667 8.533333-14.933333 19.2-14.933334 32s4.266667 23.466667 14.933334 32l384 341.333333c8.533333 6.4 19.2 10.666667 27.733333 10.666667 12.8 0 23.466667-4.266667 32-14.933333 14.933333-17.066667 14.933333-44.8-4.266667-59.733334L384 512z" p-id="14596"/>
        </svg>
        <span class="text-small">上一篇</span>
      </div>
      <div>缓存算法</div>
    </a>
  
  <div class="vhr"></div>
  
    <a class="post-nav-item-right" href="/archives/45330/">
      <div class="text-align">
        <span class="text-small">下一篇</span>
        <svg t="1670570876164"
             class="icon"
             viewBox="0 0 1024 1024"
             transform="scale(-1,-1)"
             width="16"
             height="16">
          <path d="M384 512L731.733333 202.666667c17.066667-14.933333 19.2-42.666667 4.266667-59.733334-14.933333-17.066667-42.666667-19.2-59.733333-4.266666l-384 341.333333c-10.666667 8.533333-14.933333 19.2-14.933334 32s4.266667 23.466667 14.933334 32l384 341.333333c8.533333 6.4 19.2 10.666667 27.733333 10.666667 12.8 0 23.466667-4.266667 32-14.933333 14.933333-17.066667 14.933333-44.8-4.266667-59.733334L384 512z" p-id="14596"/>
        </svg>
      </div>
      wsl杂项
    </a>
  
</div>

    
      <div class="related-post">
  <div class="related__head">
    
  
    <a href="/tags/%E7%9F%A9%E9%98%B5/" class="post-tag">#矩阵</a>
  


  </div>
  <div class="realated__body">
    
      <div class="null"><div class="null-item"><div class="null-title"><a href="\archives\33031\" title="归约证明" rel="bookmark">归约证明</a></div></div><div class="null-item"><div class="null-title"><a href="\archives\16976\" title="线性代数：厄米特矩阵、酉矩阵" rel="bookmark">线性代数：厄米特矩阵、酉矩阵</a></div></div></div>
    
  </div>
</div>

    
    
      <div id="gitalk-container"></div>
    
  </div>

    </div>
    <div class="foot">
      <div class="foot-inner">
        <div class="foot__head">
          
            <div class="foot-line">
              
                <div class="matts">海</div>
              
                <div class="matts">内</div>
              
                <div class="matts">存</div>
              
                <div class="matts">知</div>
              
                <div class="matts">己</div>
              
            </div>
          
            <div class="foot-line">
              
                <div class="matts">天</div>
              
                <div class="matts">涯</div>
              
                <div class="matts">若</div>
              
                <div class="matts">比</div>
              
                <div class="matts">邻</div>
              
            </div>
          
        </div>
        <div class="foot__body">
          
            <div class="foot-item">
              <div class="foot-item__head">友链</div>
              <div class="foot-item__body">
                
                  <div class="text">
                    <img alt="link" height="20px" width="20px" src="/assets/icon/icon-link.svg"/>
                    <a class="foot-link" target="_blank" rel="noopener" href="https://shennoter.top/">Shennoter</a>
                  </div>
                
                  <div class="text">
                    <img alt="link" height="20px" width="20px" src="/assets/icon/icon-link.svg"/>
                    <a class="foot-link" target="_blank" rel="noopener" href="https://blog.kasuganoharuka.com/">Jobove</a>
                  </div>
                
              </div>
            </div>
          
          
            <div class="foot-item">
              <div class="foot-item__head">账号</div>
              <div class="foot-item__body">
                
                  <div class="text">
                    <img alt="link" height="20px" width="20px" src="/assets/logo/logo-github.svg"/>
                    <a class="foot-link" target="_blank" rel="noopener" href="https://github.com/Lynchrocket">Lynchrocket</a>
                  </div>
                
                  <div class="text">
                    <img alt="link" height="20px" width="20px" src="/assets/logo/logo-zh.svg"/>
                    <a class="foot-link" target="_blank" rel="noopener" href="https://www.zhihu.com/people/lynchrocket">Lynchrocket</a>
                  </div>
                
              </div>
            </div>
          
          <div class="foot-item">
            <div class="foot-item__head">联系</div>
            <div class="foot-item__body">
              <div class="text">
                <img alt="link" height="20px" width="20px" src="/assets/icon/icon-email.svg"/>
                <a class="foot-link" href="mailto:lynchrocket@gmail.com">lynchrocket@gmail.com</a>
              </div>
            </div>
          </div>
          
          <div class="foot-item">
            <div class="foot-item__head">访客</div>
            <div class="foot-item__body">
              <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
              <span id="busuanzi_container_site_uv">本站总访问量：<span id="busuanzi_value_site_uv"></span>次</span>
            </div>
          </div>
          
        </div>
        <div class="copyright">
          <a href="https://lynchrocket.github.io">C'est la vie</a> &nbsp;|
          &nbsp;Powered by &nbsp;<a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>&nbsp; &
          &nbsp;<a target="_blank" rel="noopener" href="https://github.com/Lynchrocket/hexo-theme-senerity">Senerity</a>&nbsp;
        </div>
      </div>
    </div>
    
      <script src="https://unpkg.com/js-polyfills@0.1.43/es6.js"></script>
      <script id="MathJax-script"
              async
              src="https://www.unpkg.com/mathjax@3.2.2/es5/tex-mml-chtml.js"></script>
    
    
      <script src="/js/search.js"></script>
      <script>searchInitialize("/search.json")</script>
    
    
      <script src="/js/copy-code.js"></script>
    
    
  
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
<script type="text/javascript">
  const param = JSON.parse('{"enable":true,"owner":"Lynchrocket","admin":"Lynchrocket","repo":"Lynchrocket.github.io","clientID":"45d8817d291626df7894","clientSecret":"2761427f37f733d92cc64be2e2a3826cc8454cf3","distractionFreeMode":false,"proxy":"https://worker-lingering-hat-47a4.lynchrocket.workers.dev/?https://github.com/login/oauth/access_token","language":"zh-CN","per_page":20}')
  param.id = location.pathname
  const gitalk = new Gitalk(param)
  gitalk.render('gitalk-container')
</script>

  

  </body>
</html>